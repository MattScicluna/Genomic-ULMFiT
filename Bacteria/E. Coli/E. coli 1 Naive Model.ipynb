{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. coli Promoter Classification Naive Model\n",
    "\n",
    "In this notebook we train a classification model using only the promoter dataset - no language model pretraining. This serves as a baseline to verify the effect of unsupervised pretraining.\n",
    "\n",
    "This notebook also elaborates on how genomic data is process for the classification model. Many functions in the `utils.py` file are explained here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# sets device for model and PyTorch tensors\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\" # \"-1\" if want to run on a CPU, otherwise define the GPU number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai import *\n",
    "from fastai.text import ( BaseTokenizer, Tokenizer, Vocab, \n",
    "                         PreProcessor, ItemList, PathOrStr, \n",
    "                         DataFrame, Optional, Collection, \n",
    "                         IntsOrStrs, DataBunch, is_listy, \n",
    "                         ItemLists, TextList, SortishSampler, \n",
    "                         DataLoader, SortSampler, partial, pad_collate )\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import Seq\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqFeature import FeatureLocation, CompoundLocation\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../..\")\n",
    "from utils import _get_genomic_processor, get_model_clas, TextClasDataBunch, get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/mnt/wd_4tb/shared_disk_wd4tb/mattscicluna/data/genomic_ulmfit/ecoli/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = pd.read_csv(path/'e_coli_promoters_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only input to the model will be sequence data from -150/50 from defined TSS locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>Locus</th>\n",
       "      <th>Location</th>\n",
       "      <th>Sample Location</th>\n",
       "      <th>Orientation</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Promoter</th>\n",
       "      <th>Independent</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['yaaX']</td>\n",
       "      <td>['b0005']</td>\n",
       "      <td>[5233:5530](+)</td>\n",
       "      <td>[5133:5283](+)</td>\n",
       "      <td>forward</td>\n",
       "      <td>GTAACTTAGAGATTAGGATTGCGGAGAATAACAACCGCCGTTCTCA...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['talB']</td>\n",
       "      <td>['b0008']</td>\n",
       "      <td>[8237:9191](+)</td>\n",
       "      <td>[8137:8287](+)</td>\n",
       "      <td>forward</td>\n",
       "      <td>ATAACCGTCTTGTCGGCGGTTGCGCTGACGTTGCGTCGTGATATCA...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['yaaW']</td>\n",
       "      <td>['b0011']</td>\n",
       "      <td>[10642:11356](-)</td>\n",
       "      <td>[11306:11456](-)</td>\n",
       "      <td>reverse</td>\n",
       "      <td>TCAAAAATCACCTTTTCGGGTCATACGGTGAACTCATCGGATATGG...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['hokC']</td>\n",
       "      <td>['b4412']</td>\n",
       "      <td>[16750:16903](-)</td>\n",
       "      <td>[16853:17003](-)</td>\n",
       "      <td>reverse</td>\n",
       "      <td>AAAGCCCCGAGTGATATTTTACCATCAACCCGAGGCCTCCTATATG...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['nhaA']</td>\n",
       "      <td>['b0019']</td>\n",
       "      <td>[17488:18655](+)</td>\n",
       "      <td>[17388:17538](+)</td>\n",
       "      <td>forward</td>\n",
       "      <td>TAAAAAACGAATTATTCCTACACTATAATCTGATTTTAACGATGAT...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Gene      Locus          Location   Sample Location Orientation  \\\n",
       "0  ['yaaX']  ['b0005']    [5233:5530](+)    [5133:5283](+)     forward   \n",
       "1  ['talB']  ['b0008']    [8237:9191](+)    [8137:8287](+)     forward   \n",
       "2  ['yaaW']  ['b0011']  [10642:11356](-)  [11306:11456](-)     reverse   \n",
       "3  ['hokC']  ['b4412']  [16750:16903](-)  [16853:17003](-)     reverse   \n",
       "4  ['nhaA']  ['b0019']  [17488:18655](+)  [17388:17538](+)     forward   \n",
       "\n",
       "                                            Sequence  Promoter  Independent  \\\n",
       "0  GTAACTTAGAGATTAGGATTGCGGAGAATAACAACCGCCGTTCTCA...         1        False   \n",
       "1  ATAACCGTCTTGTCGGCGGTTGCGCTGACGTTGCGTCGTGATATCA...         1        False   \n",
       "2  TCAAAAATCACCTTTTCGGGTCATACGGTGAACTCATCGGATATGG...         1        False   \n",
       "3  AAAGCCCCGAGTGATATTTTACCATCAACCCGAGGCCTCCTATATG...         1        False   \n",
       "4  TAAAAAACGAATTATTCCTACACTATAATCTGATTTTAACGATGAT...         1        False   \n",
       "\n",
       "     set  \n",
       "0  train  \n",
       "1  train  \n",
       "2  train  \n",
       "3  train  \n",
       "4  train  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = classification_df[classification_df.set == 'train']\n",
    "valid_df = classification_df[classification_df.set == 'valid']\n",
    "test_df = classification_df[classification_df.set == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6791, 9), (750, 9), (830, 9))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Numericalization\n",
    "\n",
    "Before feeding genomic sequence data to the model, it needs to be processed into tokens. For this, we use the `GenomicTokenizer` class. This subclasses the `BaseTokenizer` class in the fastai library.The genomic sequences are tokenized by breaking the sequence into k-mers. The k-mer tokenization is defined by two parameters - `ngram` and `stride`. `ngram` determines the length of the k-mers. `stride` defines how far the k-mer window moves between k-mers. Empirically I have found that a k-mer size of 5 with a stride of 2 works well. This creates a list of sequential tokens with 3bp of overlap between each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang='en', ngram=5, stride=2):\n",
    "        self.lang = lang\n",
    "        self.ngram = ngram\n",
    "        self.stride = stride\n",
    "        \n",
    "    def tokenizer(self, t):\n",
    "        t = t.upper()\n",
    "        if self.ngram == 1:\n",
    "            toks = list(t)\n",
    "        else:\n",
    "            toks = [t[i:i+self.ngram] for i in range(0, len(t), self.stride) if len(t[i:i+self.ngram]) == self.ngram]\n",
    "        if len(toks[-1]) < self.ngram:\n",
    "            toks = toks[:-1]\n",
    "        \n",
    "        return toks\n",
    "    \n",
    "    def add_special_cases(self, toks):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization function is passed to the fastai `Tokenizer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(GenomicTokenizer, n_cpus=1, pre_rules=[], post_rules=[], special_cases=['xxpad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once tokens are created, they are numericalized with the `GenomicVocab` class. This class holds two objects: `itos`, a list of each k-mer in the model's vocabulary, and `stoi`, a dictionary that maps each k-mer to its index in the `itos` list. This creates the k-mer to number mapping that we will use to numericalize our data. When we tokenize the sequences into 5bp tokens, we create a vocabulary of 1024 (4^5) tokens.  We also insert a padding token that will allow us to deal with sequences of different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicVocab(Vocab):\n",
    "    def __init__(self, itos):\n",
    "        self.itos = itos\n",
    "        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})\n",
    "        \n",
    "    @classmethod\n",
    "    def create(cls, tokens, max_vocab, min_freq):\n",
    "        freq = Counter(p for o in tokens for p in o)\n",
    "        itos = [o for o,c in freq.most_common(max_vocab) if c >= min_freq]\n",
    "        itos.insert(0, 'pad')\n",
    "        return cls(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and numericalizing are handled by the `PreProcessor` class. Specifically we have the `GenomicTokenizeProcessor` for tokenization and the `GenomicNumericalizeProcessor` for numericalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicTokenizeProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that tokenizes the texts in `ds`.\"\n",
    "    def __init__(self, ds:ItemList=None, tokenizer:Tokenizer=None, chunksize:int=10000, mark_fields:bool=False):\n",
    "        self.tokenizer,self.chunksize,self.mark_fields = ifnone(tokenizer, Tokenizer()),chunksize,mark_fields\n",
    "\n",
    "    def process_one(self, item):  \n",
    "        return self.tokenizer._process_all_1(_genomic_join_texts([item], self.mark_fields))[0]\n",
    "    \n",
    "    def process(self, ds):\n",
    "        ds.items = _genomic_join_texts(ds.items, self.mark_fields)\n",
    "        tokens = []\n",
    "        for i in range(0,len(ds),self.chunksize):\n",
    "            tokens += self.tokenizer.process_all(ds.items[i:i+self.chunksize])\n",
    "        ds.items = tokens\n",
    "        \n",
    "class GenomicNumericalizeProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that numericalizes the tokens in `ds`.\"\n",
    "    def __init__(self, ds:ItemList=None, vocab:Vocab=None, max_vocab:int=60000, min_freq:int=3):\n",
    "        vocab = ifnone(vocab, ds.vocab if ds is not None else None)\n",
    "        self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq\n",
    "\n",
    "    def process_one(self,item): return np.array(self.vocab.numericalize(item), dtype=np.int64)\n",
    "    def process(self, ds):\n",
    "        if self.vocab is None: self.vocab = GenomicVocab.create(ds.items, self.max_vocab, self.min_freq)\n",
    "        ds.vocab = self.vocab\n",
    "        super().process(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these functions to process the input dataframe is handled by the `GenomicTextClasDataBunch` class. This takes as input the training data, the tokenizer, and a vocabulary if one is predefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicTextClasDataBunch(TextClasDataBunch):\n",
    "    @classmethod\n",
    "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "                label_cols:IntsOrStrs=0, label_delim:str=None, chunksize:int=10000, max_vocab:int=60000,\n",
    "                min_freq:int=2, mark_fields:bool=False, pad_idx=0, pad_first=True, bs=64, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from DataFrames. `kwargs` are passed to the dataloader creation.\"\n",
    "        processor = _get_genomic_processor(tokenizer=tokenizer, vocab=vocab, chunksize=chunksize, max_vocab=max_vocab,\n",
    "                                   min_freq=min_freq, mark_fields=mark_fields)\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        src = src.label_from_df(cols=label_cols, classes=classes, label_delim=label_delim)\n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        d1 = src.databunch(**kwargs)\n",
    "        \n",
    "        datasets = cls._init_ds(d1.train_ds, d1.valid_ds, d1.test_ds)\n",
    "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=False)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **kwargs)\n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=bs, sampler=sampler, **kwargs))\n",
    "            \n",
    "        return cls(*dataloaders, path=path, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300 # 300\n",
    "data_clas = GenomicTextClasDataBunch.from_df(path, train_df, valid_df, test_df=test_df, tokenizer=tok, \n",
    "                                            text_cols='Sequence', label_cols='Promoter', bs=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "As discussed, the genomic sequences for the promoters are split into 5-mers with a stride of 2. Here we can see the raw tokens and their numericalized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text [ 649  584  995 1006 ...  683  102   15  522], Category 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clas.train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the integers associated with each k-mer. When these integers are fed to the model, they map to specific rows of the embedding matrix. The embedding contains vector representations of each k-mer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 649,  584,  995, 1006, ...,  683,  102,   15,  522])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clas.train_ds[0][0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model\n",
    "\n",
    "The model used is based on the [AWD-LSTM](https://arxiv.org/abs/1708.02182) model. The model consists of an embedding layer, followed by a stack of LSTM layers, followed by several linear layers. The configuration I typically use is defined by the `clas_config` dictionary below. \n",
    "\n",
    "The embedding vectors are of length 400. The LSTM section consists of three stacked layers with a hidden size of 1150 units. The final linear section is defined by the `PoolingLinearClassifier` class in the fastai library. The output from the LSTM layers contain each hidden state from each k-mer. The linear section takes the final hidden state, as well as the max pooling vector over the hidden states and the average pooling vector over the hidden states. These three vectors are concatenated together and sent through two standard linear layers. The output is a vector of length 2, representing probabilities of the input being a promoter or not a promoter.\n",
    "\n",
    "Regularization is applied to the model following the implementation in the AWD-LSTM paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 400 #400\n",
    "n_hid = 1150 # 1150\n",
    "n_layers = 3 # 3\n",
    "clas_config = dict(emb_sz=embedding_size, n_hid=n_hid, n_layers=n_layers, pad_token=0, qrnn=False, output_p=0.4, \n",
    "                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)\n",
    "drop_mult = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_model_clas(data_clas, drop_mult, clas_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder, model = get_model_clas(data_clas, drop_mult, clas_config)\n",
    "#import numpy as np\n",
    "#total_params = 0\n",
    "#for p in model.parameters():\n",
    "#    total_params += np.prod(p.shape)\n",
    "#print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(1025, 400, padding_idx=0)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(1025, 400, padding_idx=0)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1150, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1150, 1150, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1150, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.08000000000000002, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained for several epochs following the learning rate and momentum scheduling outlined in [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay](https://arxiv.org/abs/1803.09820)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 2e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 5e-4, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 1e-4, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/wd_4tb/shared_disk_wd4tb/mattscicluna/data/genomic_ulmfit/ecoli/models/coli_naive2.pth')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('coli_naive', return_path=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the trained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = GenomicTextClasDataBunch.from_df(path, train_df, test_df, tokenizer=tok, \n",
    "                                            text_cols='Sequence', label_cols='Promoter', bs=300)\n",
    "learn.data = data_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves good accuracy, precision, and recall, but can be extremely unstable. Observe how the accuracy can fluctuate sharply between epochs. Several runs were needed to get the results above. Bad initializations result in the model never actually training. In the next notebook, we will see how pretraining can significantly stabilize the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genomic_ulmfit)",
   "language": "python",
   "name": "genomic_ulmfit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
