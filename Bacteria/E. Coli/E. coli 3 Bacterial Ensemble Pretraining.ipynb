{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. coli Promoter Classification with Genomic Ensemble Pretraining\n",
    "\n",
    "This notebook follows on from the previous. Here we train a promoter classification model in the same way as before. The only thing we change is initializing the classification model with weights from a better language model. The previous language model was trained on just the E. coli genome. It trained to a validation loss of 2.63. The new model is trained on an ensemble of different bacterial genomes, which trained to a validation loss of 2.50. We will see that this has a large impact on performance.\n",
    "\n",
    "To avoid clutter, confusion, and long notebooks, I separated the training of the new language model to a different notebook. The data preparation for the bacterial ensemble is found in the [Bacterial Ensemble Data Processing](https://github.com/kheyer/Genomic-ULMFiT/blob/master/Bacteria/Bacterial%20Ensemble/Genomic%20Language%20Models/Bacterial%20Ensemble%20LM%200%20Data%20Processing.ipynb) notebook. Training of the new language model is detailed in the [Bacterial Ensemble Language Model](https://github.com/kheyer/Genomic-ULMFiT/blob/master/Bacteria/Bacterial%20Ensemble/Genomic%20Language%20Models/Bacterial%20Ensemble%20LM%201%205-mer%20Language%20Model.ipynb) notebook.\n",
    "\n",
    "This notebook shows how using a better trained language model as the basis for a classification model has a significant effect on performance - much more so than training longer on classification data. This method really shines when you have a lot of unlabeled genomic data to throw at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "# sets device for model and PyTorch tensors\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\" # \"-1\" if want to run on a CPU, otherwise define the GPU number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import ( BaseTokenizer, Tokenizer, Vocab, \n",
    "                         PreProcessor, ItemList, PathOrStr, \n",
    "                         DataFrame, Optional, Collection, \n",
    "                         IntsOrStrs, DataBunch, is_listy, \n",
    "                         ItemLists, TextList, SortishSampler, \n",
    "                         DataLoader, SortSampler, partial, \n",
    "                         pad_collate, TextLMDataBunch, data_collate, \n",
    "                         LanguageModelPreLoader )\n",
    "from Bio import Seq\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqFeature import FeatureLocation, CompoundLocation\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../..\")\n",
    "from utils import ( _get_genomic_processor, get_model_clas, TextClasDataBunch, \n",
    "                    get_scores, split_data, GenomicTokenizer, get_model_LM, \n",
    "                   GenomicVocab, GenomicTextClasDataBunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/mnt/wd_4tb/shared_disk_wd4tb/mattscicluna/data/genomic_ulmfit/ecoli/')\n",
    "path_bact = Path('/mnt/wd_4tb/shared_disk_wd4tb/mattscicluna/data/genomic_ulmfit/bacterial_genomes/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = pd.read_csv(path/'e_coli_promoters_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = classification_df[classification_df.set == 'train']\n",
    "valid_df = classification_df[classification_df.set == 'valid']\n",
    "test_df = classification_df[classification_df.set == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a dataloader with the vocabulary used for the bacterial ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = np.load(path_bact/'bact_vocab.npy')\n",
    "model_vocab = GenomicVocab(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(GenomicTokenizer, n_cpus=1, pre_rules=[], post_rules=[], special_cases=['xxpad'])\n",
    "data_clas = GenomicTextClasDataBunch.from_df(path, train_df, valid_df, tokenizer=tok, vocab=model_vocab,\n",
    "                                            text_cols='Sequence', label_cols='Promoter', bs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification model is structured exactly the same as in previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=0, qrnn=False, output_p=0.4, \n",
    "                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)\n",
    "drop_mult = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_model_clas(data_clas, drop_mult, clas_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load_encoder('b2_enc')\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we follow the same training procedure as before. After 3 epochs on the training data, this model is performing better than the previous. This is entirely due to starting with a better language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, 2e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(3, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(3, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(5, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, slice(1e-4/(2.6**4),1e-4), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('coli_bact_pretrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = GenomicTextClasDataBunch.from_df(path, train_df, test_df, tokenizer=tok, vocab=model_vocab,\n",
    "                                            text_cols='Sequence', label_cols='Promoter', bs=300)\n",
    "learn.data = data_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bacterial ensemble pretrained model easily beats the E. coli genome pretrained model. The E. coli pretrained model trained to a validation loss of 0.24, while the ensemble pretrained model trained to a validation loss of \n",
    "\n",
    "Comparing this model to the E. coli pretrained model, accuracy increased from 91.9% to 97.3%, precision increased from 0.94 to 0.98, recall increased from 0.89 to 0.96, and MCC increased from 0.839 to 0.947.\n",
    "\n",
    "The models were trained on the classification dataset for the same number of epochs. The increased performance shows the impact of a well trained language model on this technique. In addition to improved performance, the ensemble pretrained model overfit less and required less regularization (lower dropout)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genomic_ulmfit)",
   "language": "python",
   "name": "genomic_ulmfit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
