{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genomic BPE Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# sets device for model and PyTorch tensors\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\" # \"-1\" if want to run on a CPU, otherwise define the GPU number\n",
    "\n",
    "print('number of devices: {}'.format(torch.cuda.device_count()))\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import collections\n",
    "\n",
    "from Bio import SeqIO\n",
    "from fastai.text import *\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrary number chosen. Original true \"max sentence length\" would be 44674217\n",
    "max_sentence_len = 1100  #max size is 20480 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = Path('/mnt/wd_4tb/shared_disk_wd4tb/mattscicluna/data/genomic_ulmfit/bacterial_genomes')\n",
    "#path_to_files = path / 'genome_fastas'\n",
    "path = Path('/mnt/wd_4tb/shared_disk_wd4tb/mattscicluna/data/genomic_ulmfit/ecoli/')\n",
    "path_to_files = path / 'just_fastas'\n",
    "path_to_all_files_txt = path / 'sentencepiece_tokenizer/all_files.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(path_to_files.iterdir()):\n",
    "    genome = SeqIO.parse(file, 'fasta')\n",
    "    chroms = [GB for GB in genome if ('chromosome' in GB.description) or ('complete genome' in GB.description)] #  remove plasmid\n",
    "    genome = ''.join([i.seq.__str__() for i in chroms]).upper()\n",
    "    genome += '\\n'\n",
    "    with open(path_to_all_files_txt, 'a') as f:\n",
    "        ind = 0\n",
    "        while ind < len(genome):\n",
    "            f.write(genome[ind:ind+max_sentence_len])\n",
    "            f.write('\\n')\n",
    "            ind += max_sentence_len\n",
    "    print('completed file {}/{}'.format(i,len(path_to_files.ls())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotemark = '\\\"'\n",
    "raw_text_path = path_to_all_files_txt\n",
    "lang = 'dna'\n",
    "pre_rules=None\n",
    "post_rules=None\n",
    "vocab_sz=None\n",
    "max_vocab_sz = 10000\n",
    "vocab_size = 100\n",
    "model_type='bpe'\n",
    "char_coverage=None\n",
    "tmp_dir='tmp' \n",
    "enc='utf8'\n",
    "coverage = 0.9998\n",
    "spec_tokens = ['\\u2581'+s for s in defaults.text_spec_tok]\n",
    "spec_tokens.extend([str(i) for i in range(1, 21)])\n",
    "cache_dir = path / 'sentencepiece_tokenizer' \n",
    "cache_dir = cache_dir / tmp_dir\n",
    "\n",
    "#changed line in fastai.text see: https://forums.fast.ai/t/multifit-runtime-error-permission-denied/72874/3\n",
    "# in the end not creating model thru fastai. using sentencepiece package directly!\n",
    "print(\" \".join([\n",
    "        f\"--input={quotemark}{raw_text_path}{quotemark} --max_sentence_length={max_sentence_len}\",\n",
    "        f\"--character_coverage={coverage}\",\n",
    "        f\"--unk_id={len(defaults.text_spec_tok)} --pad_id=-1 --bos_id=-1 --eos_id=-1\",\n",
    "        f\"--user_defined_symbols={','.join(spec_tokens)}\",\n",
    "        f\"--model_prefix={cache_dir/'spm'} --vocab_size={vocab_size} --model_type={model_type}\"]))\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\" \".join([\n",
    "        f\"--input={quotemark}{raw_text_path}{quotemark} --max_sentence_length={max_sentence_len}\",\n",
    "        f\"--character_coverage={coverage}\",\n",
    "        f\"--unk_id={len(defaults.text_spec_tok)} --pad_id=-1 --bos_id=-1 --eos_id=-1\",\n",
    "        f\"--user_defined_symbols={','.join(spec_tokens)}\",\n",
    "        f\"--model_prefix={cache_dir/'spm'} --vocab_size={vocab_size} --model_type={model_type}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file='/mnt/wd_4tb/shared_disk_wd4tb/mattscicluna/data/genomic_ulmfit/bacterial_genomes/sentencepiece_tokenizer/tmp/spm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = [[s.id_to_piece(id), id] for id in range(s.get_piece_size())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs[0:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test_text = \"\".join([random.choice(\"ATCG\") for _ in range(1000)])\n",
    "\n",
    "print(test_text)\n",
    "print(s.encode(test_text, out_type=str, enable_sampling=True, alpha=0.1)[0:10])\n",
    "print(s.encode(test_text, out_type=int, enable_sampling=True, alpha=0.1)[0:10])\n",
    "\n",
    "test_text_2 = \"1 2 3 4\"\n",
    "\n",
    "print(test_text_2)\n",
    "print(s.encode(test_text_2, out_type=str, enable_sampling=True, alpha=0.1)[0:10])\n",
    "print(s.encode(test_text_2, out_type=int, enable_sampling=True, alpha=0.1)[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Trying out model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Sentencepiece model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import ( BaseTokenizer, Tokenizer, Vocab, \n",
    "                         PreProcessor, ItemList, PathOrStr, \n",
    "                         DataFrame, Optional, Collection, \n",
    "                         IntsOrStrs, DataBunch, is_listy, \n",
    "                         ItemLists, TextList, SortishSampler, \n",
    "                         DataLoader, SortSampler, partial, \n",
    "                         pad_collate, TextLMDataBunch, data_collate, \n",
    "                         LanguageModelPreLoader, SPProcessor, PreProcessor )\n",
    "from Bio import Seq\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqFeature import FeatureLocation, CompoundLocation\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append(\"../..\")\n",
    "from utils import ( _get_genomic_processor, get_model_clas, TextClasDataBunch, \n",
    "                    get_scores, split_data, GenomicTokenizer, get_model_LM, \n",
    "                   GenomicVocab, GenomicTextClasDataBunch, GenomicTextLMDataBunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/mnt/wd_4tb/shared_disk_wd4tb/mattscicluna/data/genomic_ulmfit/ecoli/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'e_coli_lm_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = split_data(df, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_(text):\n",
    "    return text.replace('_', '').strip()\n",
    "\n",
    "def remove_from_list(texts):\n",
    "    #print('texts are: {}'.format(texts))\n",
    "    return [remove_single(text) for text in texts]\n",
    "\n",
    "def remove_single(text):\n",
    "    #print('applying ...')\n",
    "    #text = text.replace('xxup ', '') # remove uppercase indicator (useless)\n",
    "    text = text.upper() # make everything uppercase\n",
    "    text = text.replace('XXBOS', 'xxbos') # undo uppercasing BOS\n",
    "    text = text.replace('XXEOS', 'xxeos') # undo uppercasing EOS\n",
    "    #text = text.replace('XXUP', 'xxup')\n",
    "    #text = text.replace('XXREP', 'xxrep') # undo uppercasing REP\n",
    "    return text\n",
    "\n",
    "\n",
    "tmp_dir='tmp' \n",
    "cache_dir = path / 'sentencepiece_tokenizer' \n",
    "cache_dir = cache_dir / tmp_dir\n",
    "\n",
    "gen_sp_processor = SPProcessor(sp_model=cache_dir/'spm.model', \n",
    "                               sp_vocab=cache_dir/'spm.vocab',\n",
    "                               pre_rules=[remove_], \n",
    "                               post_rules=[remove_from_list], \n",
    "                               model_type='bpe',\n",
    "                               max_sentence_len=20480, \n",
    "                               lang='dna', \n",
    "                               char_coverage=0.9998, \n",
    "                               tmp_dir='tmp', \n",
    "                               mark_fields=False, \n",
    "                               include_bos=True, \n",
    "                               include_eos=True,  \n",
    "                               enc='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicTextLMDataBunchSP(TextLMDataBunch):\n",
    "    @classmethod\n",
    "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "                processor:PreProcessor=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "                label_cols:IntsOrStrs=0, label_delim:str=None, bptt=70, collate_fn:Callable=data_collate, bs=64, **kwargs):\n",
    "        \"Create a `TextDataBunch` from DataFrames. `kwargs` are passed to the dataloader creation.\"\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        src = src.label_for_lm() \n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        d1 = src.databunch(**kwargs)\n",
    "        \n",
    "        datasets = cls._init_ds(d1.train_ds, d1.valid_ds, d1.test_ds)            \n",
    "        val_bs = bs\n",
    "        datasets = [LanguageModelPreLoader(ds, shuffle=(i==0), bs=(bs if i==0 else val_bs), bptt=bptt, backwards=False) \n",
    "                    for i,ds in enumerate(datasets)]            \n",
    "        dls = [DataLoader(d, b, shuffle=False) for d,b in zip(datasets, (bs,val_bs,val_bs,val_bs)) if d is not None]\n",
    "        \n",
    "        return cls(*dls, path=path, collate_fn=collate_fn, no_check=False)\n",
    "\n",
    "data = GenomicTextLMDataBunchSP.from_df(path, train_df, valid_df, bs=428, processor=gen_sp_processor, text_cols=0, label_cols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "\n",
    "for i, dat in enumerate(data.train_dl):\n",
    "    c.update(dat[0].reshape(-1).tolist())\n",
    "    print('done {} of {}'.format(i, len(data.train_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('{} tokens appear in DNA'.format(len(c)))\n",
    "most_common = 'most common: \\n'\n",
    "most_common_items = c.most_common()\n",
    "for i in range(10):\n",
    "    print('{}: {}'.format(gen_sp_processor.vocab.textify([most_common_items[i][0]]), most_common_items[i][1]))\n",
    "\n",
    "lens = [len(x) for x in gen_sp_processor.vocab.textify(list(c.keys())).split(' ')]\n",
    "print('avg len: {:.3f}'.format(np.array(lens).mean()))\n",
    "print('{} is the lowest amount of times any token appears'.format(np.array(list(c.values())).min()))\n",
    "#TODO: compute average length of tokens!\n",
    "\n",
    "plt.hist(lens, bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(c.values())\n",
    "x.sort(reverse=True)\n",
    "\n",
    "plt.plot(x)\n",
    "\n",
    "print('num tokens: {}'.format(len(data.vocab.itos)))\n",
    "print('first sequence: {}'.format(train_df.iloc[0].Sequence))\n",
    "print('encoded: {}'.format(data.train_ds[0]))\n",
    "encoding = gen_sp_processor.process_one(train_df.iloc[0].Sequence)\n",
    "print('decoded: {}'.format(gen_sp_processor.vocab.textify(encoding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.vocab.itos[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path/'coli_vocab_sp.npy', data.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=0, qrnn=False, output_p=0.25, \n",
    "                          hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15, tie_weights=True, out_bias=True)\n",
    "drop_mult = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_model_LM(data, drop_mult, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 1e-1, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, 5e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('coli_only_LM_sp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('coli_only_LM_enc_sp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification downstream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/mnt/wd_4tb/shared_disk_wd4tb/mattscicluna/data/genomic_ulmfit/ecoli/')\n",
    "classification_df = pd.read_csv(path/'e_coli_promoters_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = classification_df[classification_df.set == 'train']\n",
    "valid_df = classification_df[classification_df.set == 'valid']\n",
    "test_df = classification_df[classification_df.set == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_(text):\n",
    "    return text.replace('_', '').strip()\n",
    "\n",
    "def remove_from_list(texts):\n",
    "    #print('texts are: {}'.format(texts))\n",
    "    return [remove_single(text) for text in texts]\n",
    "\n",
    "def remove_single(text):\n",
    "    #print('applying ...')\n",
    "    #text = text.replace('xxup ', '') # remove uppercase indicator (useless)\n",
    "    text = text.upper() # make everything uppercase\n",
    "    text = text.replace('XXBOS', 'xxbos') # undo uppercasing BOS\n",
    "    text = text.replace('XXEOS', 'xxeos') # undo uppercasing EOS\n",
    "    #text = text.replace('XXUP', 'xxup')\n",
    "    #text = text.replace('XXREP', 'xxrep') # undo uppercasing REP\n",
    "    return text\n",
    "\n",
    "\n",
    "tmp_dir='tmp' \n",
    "cache_dir = path / 'sentencepiece_tokenizer' \n",
    "cache_dir = cache_dir / tmp_dir\n",
    "\n",
    "gen_sp_processor = SPProcessor(sp_model=cache_dir/'spm.model', \n",
    "                               sp_vocab=cache_dir/'spm.vocab',\n",
    "                               pre_rules=[remove_], \n",
    "                               post_rules=[remove_from_list], \n",
    "                               model_type='bpe',\n",
    "                               max_sentence_len=20480, \n",
    "                               lang='dna', \n",
    "                               char_coverage=0.9998, \n",
    "                               tmp_dir='tmp', \n",
    "                               mark_fields=False, \n",
    "                               include_bos=True, \n",
    "                               include_eos=True,  \n",
    "                               enc='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicTextClasDataBunchSP(TextClasDataBunch):\n",
    "    @classmethod\n",
    "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "                processor:PreProcessor=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "                label_cols:IntsOrStrs=0, pad_idx=1, pad_first=True, label_delim:str=None,\n",
    "                bs=64, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from DataFrames. `kwargs` are passed to the dataloader creation.\"\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        src = src.label_from_df(cols=label_cols, classes=classes, label_delim=label_delim)\n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        d1 = src.databunch(**kwargs)\n",
    "        \n",
    "        datasets = cls._init_ds(d1.train_ds, d1.valid_ds, d1.test_ds)\n",
    "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=False)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **kwargs)\n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=bs, sampler=sampler, **kwargs))\n",
    "            \n",
    "        return cls(*dataloaders, path=path, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok = Tokenizer(GenomicTokenizer, n_cpus=1, pre_rules=[], post_rules=[], special_cases=['xxpad'])\n",
    "data_clas = GenomicTextClasDataBunchSP.from_df(path, train_df, valid_df, processor=gen_sp_processor,\n",
    "                                             text_cols='Sequence', label_cols='Promoter', bs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=0, qrnn=False, output_p=0.4, \n",
    "                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)\n",
    "drop_mult = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_model_clas(data_clas, drop_mult, clas_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load_encoder('coli_only_LM_enc_sp')\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, 1e-1, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(3, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(3, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(5, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, slice(1e-4/(2.6**4),1e-4), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('coli_coli_pretrain_sp', return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = GenomicTextClasDataBunchSP.from_df(path, train_df, test_df, processor=gen_sp_processor,\n",
    "                                            text_cols='Sequence', label_cols='Promoter', bs=300)\n",
    "learn.data = data_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genomic_ulmfit)",
   "language": "python",
   "name": "genomic_ulmfit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
